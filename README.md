### PT-BR

# üöÄ Projeto Completo de Azure Databricks

Este projeto foi **desenvolvido por mim com foco em cen√°rios reais de Engenharia de Dados**, utilizando o **Azure Databricks** como plataforma principal.  
O objetivo foi criar uma solu√ß√£o **end-to-end (E2E)** totalmente funcional ‚Äî cobrindo desde a **ingest√£o e transforma√ß√£o de dados** at√© a **modelagem dimensional e integra√ß√£o com ferramentas de BI**, aplicando **boas pr√°ticas de governan√ßa, seguran√ßa e performance**.

---

## üß† Objetivo do Projeto
Desenvolver uma **pipeline de dados completa** que reflita o que √© feito em **ambientes corporativos reais**, explorando todos os recursos modernos do ecossistema Databricks.  
Este projeto tamb√©m foi criado para **refor√ßar meus conhecimentos t√©cnicos** e servir como **prova pr√°tica de dom√≠nio em Engenharia de Dados**, especialmente para entrevistas t√©cnicas em 2025.

---

## üß± Arquitetura e Fluxo de Dados

A arquitetura segue o padr√£o **Medallion (Bronze ‚Üí Silver ‚Üí Gold)**, garantindo modularidade, rastreabilidade e governan√ßa em todas as etapas.

### **1. Prepara√ß√£o e Governan√ßa**
- Criei uma conta gratuita no **Azure** e configurei o **Azure Data Lake Storage (ADLS)** como camada de armazenamento.  
- Configurei o **Unity Catalog** para gerenciar credenciais, metadados e seguran√ßa, seguindo as melhores pr√°ticas do Databricks.  
- Trabalhei com o formato **Parquet**, visando performance e efici√™ncia no processamento.

---

### **2. Camada Bronze (Ingest√£o)**
- Implementei a ingest√£o incremental usando **Spark Structured Streaming** com o **Autoloader**.  
- Configurei **checkpoint locations** para garantir idempot√™ncia e processamento ‚Äúexactly once‚Äù.  
- Armazenei os dados brutos em formato **Delta Lake**, permitindo versionamento e transa√ß√µes ACID.  

---

### **3. Camada Silver (Transforma√ß√£o)**
- Apliquei **transforma√ß√µes em PySpark** usando conceitos de **Programa√ß√£o Orientada a Objetos (POO)** para aumentar a modularidade do c√≥digo.  
- Desenvolvi **fun√ß√µes reutiliz√°veis** e as registrei no **Unity Catalog**, de forma semelhante a fun√ß√µes SQL persistentes.  
- Realizei a padroniza√ß√£o e limpeza dos dados, garantindo qualidade e consist√™ncia antes da modelagem.

---

### **4. Camada Gold (Modelagem Dimensional)**
- Modelei um **Star Schema completo**, contendo tabelas de **Dimens√µes e Fatos**.  
- Implementei **SCD Tipo 1 (Upsert)** com `MERGE INTO` para atualizar e inserir registros.  
- Criei **SCD Tipo 2 (Hist√≥rico)** utilizando **Delta Live Tables (DLT)**, configurando **Expectations** para controle e qualidade dos dados.  

---

### **5. Consumo e Visualiza√ß√£o**
- Disponibilizei os dados atrav√©s do **SQL Warehouse (Lakehouse)** para consultas otimizadas.  
- Conectei o projeto ao **Power BI** via **Partner Connect**, criando dashboards din√¢micos com dados em tempo real.  

---

## ‚öôÔ∏è Tecnologias Utilizadas
- **Azure Databricks (Runtime, DLT, SQL Warehouse)**  
- **Azure Data Lake Storage (ADLS)**  
- **Unity Catalog**  
- **PySpark / Python (POO)**  
- **Delta Lake (ACID Transactions)**  
- **Power BI**  

---

## üéØ Conceitos Abordados
- Arquitetura **Medallion (Bronze, Silver, Gold)**  
- **Ingest√£o incremental e streaming**  
- **Idempot√™ncia e Exactly Once Processing**  
- **Modelagem Dimensional (Star Schema)**  
- **Slowly Changing Dimensions (SCD Tipo 1 e 2)**  
- **Delta Live Tables e Data Quality**  
- **Governan√ßa e seguran√ßa com Unity Catalog**  
- **Integra√ß√£o BI e consumo anal√≠tico**  

---

## üß© Resultado Final
O resultado √© uma **pipeline de dados moderna, escal√°vel e governada**, que representa o fluxo completo de dados em um **Data Lakehouse corporativo**.  
Ela realiza desde a ingest√£o automatizada at√© a modelagem dimensional, com **qualidade e controle hist√≥rico garantidos**.

---

## üí° Meu Aprendizado
Durante o desenvolvimento, aprofundei meu conhecimento em:
- Estrutura√ß√£o de **camadas de dados** no Databricks;  
- Implementa√ß√£o de **cargas incrementais** e controle de hist√≥rico;  
- Uso pr√°tico de **Delta Live Tables**;  
- Boas pr√°ticas de **governan√ßa com Unity Catalog**;  
- Integra√ß√£o de **pipelines anal√≠ticos com Power BI**.  

Este projeto representa um marco importante na minha jornada para **dominar o Databricks e as pr√°ticas de Engenharia de Dados de n√≠vel enterprise**.  

---
