### PT-BR

# üöÄ Projeto Completo de Azure Databricks

Este projeto foi **desenvolvido por mim com foco em cen√°rios reais de Engenharia de Dados**, utilizando o **Azure Databricks** como plataforma principal.  
O objetivo foi criar uma solu√ß√£o **end-to-end (E2E)** totalmente funcional ‚Äî cobrindo desde a **ingest√£o e transforma√ß√£o de dados** at√© a **modelagem dimensional e integra√ß√£o com ferramentas de BI**, aplicando **boas pr√°ticas de governan√ßa, seguran√ßa e performance**.

---

## üß† Objetivo do Projeto
Desenvolver uma **pipeline de dados completa** que reflita o que √© feito em **ambientes corporativos reais**, explorando todos os recursos modernos do ecossistema Databricks.  
Este projeto tamb√©m foi criado para **refor√ßar meus conhecimentos t√©cnicos** e servir como **prova pr√°tica de dom√≠nio em Engenharia de Dados**, especialmente para entrevistas t√©cnicas em 2025.

---

## üß± Arquitetura e Fluxo de Dados

A arquitetura segue o padr√£o **Medallion (Bronze ‚Üí Silver ‚Üí Gold)**, garantindo modularidade, rastreabilidade e governan√ßa em todas as etapas.

### **1. Prepara√ß√£o e Governan√ßa**
- Criei uma conta gratuita no **Azure** e configurei o **Azure Data Lake Storage (ADLS)** como camada de armazenamento.  
- Configurei o **Unity Catalog** para gerenciar credenciais, metadados e seguran√ßa, seguindo as melhores pr√°ticas do Databricks.  
- Trabalhei com o formato **Parquet**, visando performance e efici√™ncia no processamento.

---

### **2. Camada Bronze (Ingest√£o)**
- Implementei a ingest√£o incremental usando **Spark Structured Streaming** com o **Autoloader**.  
- Configurei **checkpoint locations** para garantir idempot√™ncia e processamento ‚Äúexactly once‚Äù.  
- Armazenei os dados brutos em formato **Delta Lake**, permitindo versionamento e transa√ß√µes ACID.  

---

### **3. Camada Silver (Transforma√ß√£o)**
- Apliquei **transforma√ß√µes em PySpark** usando conceitos de **Programa√ß√£o Orientada a Objetos (POO)** para aumentar a modularidade do c√≥digo.  
- Desenvolvi **fun√ß√µes reutiliz√°veis** e as registrei no **Unity Catalog**, de forma semelhante a fun√ß√µes SQL persistentes.  
- Realizei a padroniza√ß√£o e limpeza dos dados, garantindo qualidade e consist√™ncia antes da modelagem.

---

### **4. Camada Gold (Modelagem Dimensional)**
- Modelei um **Star Schema completo**, contendo tabelas de **Dimens√µes e Fatos**.  
- Implementei **SCD Tipo 1 (Upsert)** com `MERGE INTO` para atualizar e inserir registros.  
- Criei **SCD Tipo 2 (Hist√≥rico)** utilizando **Delta Live Tables (DLT)**, configurando **Expectations** para controle e qualidade dos dados.  

---

## ‚öôÔ∏è Tecnologias Utilizadas
- **Azure Databricks (Runtime, DLT, SQL Warehouse)**  
- **Azure Data Lake Storage (ADLS)**  
- **Unity Catalog**  
- **PySpark / Python (POO)**  
- **Delta Lake (ACID Transactions)**  

---

## üéØ Conceitos Abordados
- Arquitetura **Medallion (Bronze, Silver, Gold)**  
- **Ingest√£o incremental e streaming**  
- **Idempot√™ncia e Exactly Once Processing**  
- **Modelagem Dimensional (Star Schema)**  
- **Slowly Changing Dimensions (SCD Tipo 1 e 2)**  
- **Delta Live Tables e Data Quality**  
- **Governan√ßa e seguran√ßa com Unity Catalog**  
- **Integra√ß√£o BI e consumo anal√≠tico**  

---

## üß© Resultado Final
O resultado √© uma **pipeline de dados moderna, escal√°vel e governada**, que representa o fluxo completo de dados em um **Data Lakehouse corporativo**.  
Ela realiza desde a ingest√£o automatizada at√© a modelagem dimensional, com **qualidade e controle hist√≥rico garantidos**.

---

## üí° Meu Aprendizado
Durante o desenvolvimento, aprofundei meu conhecimento em:
- Estrutura√ß√£o de **camadas de dados** no Databricks;  
- Implementa√ß√£o de **cargas incrementais** e controle de hist√≥rico;  
- Uso pr√°tico de **Delta Live Tables**;  
- Boas pr√°ticas de **governan√ßa com Unity Catalog**;  

Este projeto representa um marco importante na minha jornada para **dominar o Databricks e as pr√°ticas de Engenharia de Dados**.  

---

### EN

# üöÄ Complete Azure Databricks Project  

This project was **developed by me to simulate real-world Data Engineering scenarios** using **Azure Databricks** as the main platform.  
The goal was to build a **fully functional end-to-end (E2E) data pipeline** ‚Äî covering everything from **data ingestion and transformation** to **dimensional modeling and BI integration** ‚Äî while applying **best practices for governance, security, and performance**.

---

## üß† Project Objective
To develop a **complete enterprise-grade data pipeline** that reflects real corporate environments, leveraging all the modern features of the Databricks ecosystem.  
I created this project to **strengthen my technical expertise** and serve as **practical evidence of my proficiency in Data Engineering**, especially for **technical interviews in 2025**.

---

## üß± Architecture and Data Flow

The architecture follows the **Medallion pattern (Bronze ‚Üí Silver ‚Üí Gold)**, ensuring modularity, traceability, and strong data governance at every stage.

### **1. Preparation and Governance**
- Set up a free **Azure** account and configured **Azure Data Lake Storage (ADLS)** as the main data layer.  
- Configured **Unity Catalog** for credential management, metadata governance, and security, following real-world best practices.  
- Used **Parquet** as the base data format for performance and compression efficiency.  

---

### **2. Bronze Layer (Ingestion)**
- Implemented **incremental ingestion** using **Spark Structured Streaming** with the **Autoloader** feature.  
- Configured **checkpoint locations** to ensure idempotency and **exactly-once processing**.  
- Stored raw data in **Delta Lake** format to enable ACID transactions and version control.  

---

### **3. Silver Layer (Transformation)**
- Applied **PySpark transformations** using **Object-Oriented Programming (OOP)** principles to improve modularity.  
- Built **reusable functions** and registered them in **Unity Catalog**, similar to persistent SQL functions.  
- Standardized and cleansed the data to ensure quality and consistency before modeling.  

---

### **4. Gold Layer (Dimensional Modeling)**
- Designed a full **Star Schema**, including **Dimension** and **Fact** tables.  
- Implemented **SCD Type 1 (Upsert)** using `MERGE INTO` for updates and inserts.  
- Created **SCD Type 2 (Historical)** logic with **Delta Live Tables (DLT)**, and defined **Expectations** to enforce data quality.  

---

## ‚öôÔ∏è Technologies Used
- **Azure Databricks (Runtime, DLT, SQL Warehouse)**  
- **Azure Data Lake Storage (ADLS)**  
- **Unity Catalog**  
- **PySpark / Python (OOP)**  
- **Delta Lake (ACID Transactions)**  

---

## üéØ Key Concepts Covered
- **Medallion Architecture (Bronze, Silver, Gold)**  
- **Incremental and streaming ingestion**  
- **Idempotency and Exactly Once Processing**  
- **Dimensional Modeling (Star Schema)**  
- **Slowly Changing Dimensions (SCD Type 1 & Type 2)**  
- **Delta Live Tables and Data Quality**  
- **Data Governance and Security with Unity Catalog**  
- **BI Integration and Analytical Consumption**  

---

## üß© Final Outcome
The final result is a **modern, scalable, and fully governed data pipeline**, representing a complete data lifecycle within a **corporate-grade Data Lakehouse**.  
It performs automated ingestion, transformation, and dimensional modeling ‚Äî ensuring **data quality, history tracking, and performance optimization**.

---

## üí° What I Learned
Throughout the development, I deepened my understanding of:
- Structuring **data layers** within Databricks;  
- Implementing **incremental loads** and historical tracking;  
- Building **Delta Live Tables (DLT)** workflows;  
- Applying **Unity Catalog** for governance and reusability;  

This project marks an important milestone in my journey to **master Databricks and Data Engineering practices**.  


